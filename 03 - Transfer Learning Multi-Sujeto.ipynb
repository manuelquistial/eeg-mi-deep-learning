{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Transfer Learning: Pre-entrenamiento Multi-Sujeto con Leave-One-Subject-Out\n",
    "\n",
    "Este notebook implementa **Transfer Learning**: Pre-entrenamiento Multi-Sujeto con Fine-tuning Progresivo y Leave-One-Subject-Out (LOSO) cross-validation.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. **Pre-entrenar** el modelo CNN-LSTM con datos de múltiples sujetos (N-1 sujetos)\n",
    "2. **Fine-tune** el modelo para cada sujeto individual usando fine-tuning progresivo\n",
    "3. **Evaluar** usando Leave-One-Subject-Out cross-validation\n",
    "4. **Comparar** resultados con y sin transfer learning\n",
    "\n",
    "## Estrategia Implementada\n",
    "\n",
    "### Fase 1: Pre-entrenamiento Multi-Sujeto\n",
    "- Entrenar modelo completo con datos de 14 sujetos\n",
    "- Aprender características generales de señales EEG\n",
    "- Guardar pesos del modelo pre-entrenado\n",
    "\n",
    "### Fase 2: Fine-tuning Progresivo (por sujeto)\n",
    "- Etapa 1: Congelar CNN, fine-tune solo capas Dense\n",
    "- Etapa 2: Descongelar LSTM + Dense, fine-tune\n",
    "- Etapa 3: Descongelar últimas capas CNN + LSTM + Dense, fine-tune\n",
    "- Etapa 4: Fine-tune completo con learning rate muy bajo\n",
    "\n",
    "### Fase 3: Leave-One-Subject-Out Cross-Validation\n",
    "- Para cada sujeto (15 iteraciones):\n",
    "  - Pre-entrenar con 14 sujetos\n",
    "  - Fine-tune para el sujeto restante\n",
    "  - Evaluar en el sujeto de prueba\n",
    "- Reportar métricas promedio y por sujeto\n",
    "\n",
    "**Nota**: Este proceso puede tomar un buen tiempo en ejecutarse completamente debido a:\n",
    "- Pre-entrenamiento con múltiples sujetos\n",
    "- Fine-tuning progresivo por cada sujeto\n",
    "- 15 iteraciones de Leave-One-Subject-Out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaciones y Configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importaciones completadas\n",
      "TensorFlow version: 2.16.2\n",
      "GPU disponible: False\n"
     ]
    }
   ],
   "source": [
    "# Importaciones\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Para carga de archivos EEGLAB (.set)\n",
    "try:\n",
    "    import mne\n",
    "    mne.set_log_level('ERROR')  # Suprimir mensajes informativos\n",
    "except ImportError:\n",
    "    print(\"Instalando mne...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"mne\"])\n",
    "    import mne\n",
    "    mne.set_log_level('ERROR')  # Suprimir mensajes informativos\n",
    "\n",
    "# Función auxiliar para leer archivos EEGLAB (.set)\n",
    "def load_eeglab_file(file_path, return_all_epochs=False):\n",
    "    \"\"\"Lee archivos EEGLAB (.set) usando MNE.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo .set\n",
    "        return_all_epochs: Si True, retorna todas las epochs como lista de arrays 2D.\n",
    "                          Si False, retorna solo la primera epoch como array 2D.\n",
    "    \n",
    "    Returns:\n",
    "        Si return_all_epochs=False: array 2D (n_channels, n_times)\n",
    "        Si return_all_epochs=True: lista de arrays 2D [(n_channels, n_times), ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Método 1: Intentar leer como epochs (más común en EEGLAB)\n",
    "        try:\n",
    "            epochs = mne.read_epochs_eeglab(str(file_path), verbose=False)\n",
    "            epochs_data = epochs.get_data()  # Formato: (n_epochs, n_channels, n_times)\n",
    "            \n",
    "            if len(epochs_data.shape) == 3:\n",
    "                n_epochs, n_channels, n_times = epochs_data.shape\n",
    "                \n",
    "                if return_all_epochs:\n",
    "                    # Retornar todas las epochs como lista de arrays 2D\n",
    "                    return [epochs_data[i] for i in range(n_epochs)]\n",
    "                else:\n",
    "                    # Retornar solo la primera epoch\n",
    "                    return epochs_data[0]  # (n_channels, n_times)\n",
    "            else:\n",
    "                # Si no es 3D, intentar otro método\n",
    "                raise ValueError(f\"Formato inesperado de epochs: {epochs_data.shape}\")\n",
    "                \n",
    "        except Exception as epochs_error:\n",
    "            # Método 2: Intentar leer como raw (datos continuos)\n",
    "            try:\n",
    "                raw = mne.io.read_raw_eeglab(str(file_path), preload=True, verbose=False)\n",
    "                data = raw.get_data()  # Formato: (n_channels, n_times)\n",
    "                \n",
    "                if return_all_epochs:\n",
    "                    return [data]  # Retornar como lista con un solo elemento\n",
    "                else:\n",
    "                    return data\n",
    "                    \n",
    "            except Exception as raw_error:\n",
    "                # Método 3: Usar scipy.io.loadmat como fallback\n",
    "                from scipy.io import loadmat\n",
    "                mat = loadmat(str(file_path), simplify_cells=False)\n",
    "                \n",
    "                # Buscar estructura EEG\n",
    "                if 'EEG' not in mat:\n",
    "                    raise ValueError(f\"Campo 'EEG' no encontrado en {file_path.name}\")\n",
    "                \n",
    "                eeg_struct = mat['EEG']\n",
    "                if not isinstance(eeg_struct, np.ndarray) or eeg_struct.size == 0:\n",
    "                    raise ValueError(f\"Estructura EEG vacía en {file_path.name}\")\n",
    "                \n",
    "                # Acceder al objeto EEG\n",
    "                eeg_item = eeg_struct[0, 0] if eeg_struct.ndim >= 2 else eeg_struct.item()\n",
    "                \n",
    "                # Extraer el campo 'data'\n",
    "                if not hasattr(eeg_item, 'dtype') or not eeg_item.dtype.names or 'data' not in eeg_item.dtype.names:\n",
    "                    raise ValueError(f\"Campo 'data' no encontrado en estructura EEG\")\n",
    "                \n",
    "                data_field = eeg_item['data']\n",
    "                data = np.asarray(data_field, dtype=np.float64)\n",
    "                \n",
    "                # Manejar diferentes formatos\n",
    "                if len(data.shape) == 2:\n",
    "                    # (n_channels, n_times) o (n_times, n_channels)\n",
    "                    if return_all_epochs:\n",
    "                        return [data]\n",
    "                    else:\n",
    "                        return data\n",
    "                elif len(data.shape) == 3:\n",
    "                    # (n_epochs, n_channels, n_times)\n",
    "                    if return_all_epochs:\n",
    "                        return [data[i] for i in range(data.shape[0])]\n",
    "                    else:\n",
    "                        return data[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Formato de datos no soportado: {data.shape}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error al leer archivo {file_path}: {e}\")\n",
    "\n",
    "# Para descarga de Google Drive y manejo de ZIP\n",
    "try:\n",
    "    import gdown\n",
    "except ImportError:\n",
    "    print(\"Instalando gdown...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"gdown\"])\n",
    "    import gdown\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"Importaciones completadas\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada\n",
      "- Número de sujetos: 15\n",
      "- Épocas pre-entrenamiento: 30\n",
      "- Épocas fine-tuning (por etapa): 5, 6, 7, 8\n"
     ]
    }
   ],
   "source": [
    "# Configuración del proyecto\n",
    "CONFIG = {\n",
    "    'n_subjects': 15,\n",
    "    'n_channels': 64,\n",
    "    'n_trials_per_subject': 22,  # 11 left + 11 right según el dataset\n",
    "    'n_total_trials': 660,\n",
    "    'sampling_rate': 128,\n",
    "    'trial_duration': 9.0,\n",
    "    'trial_window': (-3.0, 6.0),\n",
    "    'n_samples_per_trial': 1152,\n",
    "    'filter_low': 8,\n",
    "    'filter_high': 30,\n",
    "    'random_seed': 42,\n",
    "    'data_dir': 'data',\n",
    "    'models_dir': 'models',\n",
    "    'results_dir': 'results',\n",
    "    \n",
    "    # Configuración de Transfer Learning\n",
    "    'pretrain_epochs': 30,  # Épocas para pre-entrenamiento (reducido para optimización)\n",
    "    'finetune_epochs_stage1': 5,  # Dense layers (muy reducido para optimización)\n",
    "    'finetune_epochs_stage2': 6,  # LSTM + Dense (muy reducido para optimización)\n",
    "    'finetune_epochs_stage3': 7,  # CNN final + LSTM + Dense (muy reducido para optimización)\n",
    "    'finetune_epochs_stage4': 8,  # Fine-tune completo (muy reducido para optimización)\n",
    "    'finetune_early_stopping_patience': 2,  # Patience muy agresivo (detener después de 2 épocas sin mejora)\n",
    "    'finetune_min_delta': 1e-4,  # Mejora mínima requerida más grande (detener antes)\n",
    "    'finetune_skip_stages': True,  # Si True, permite saltar etapas si ya convergió\n",
    "    \n",
    "    'pretrain_lr': 0.001,\n",
    "    'finetune_lr_stage1': 0.0005,\n",
    "    'finetune_lr_stage2': 0.0003,\n",
    "    'finetune_lr_stage3': 0.0001,\n",
    "    'finetune_lr_stage4': 0.00005,\n",
    "    \n",
    "    'batch_size': 128,  # Aumentado significativamente para reducir pasos por época (1133 → ~283)\n",
    "    'validation_split': 0.15,  # Reducido para más datos de entrenamiento y menos validación\n",
    "    'early_stopping_patience': 5,  # Reducido para detener más rápido\n",
    "    'pretrain_min_delta': 1e-5,  # Mejora mínima requerida para early stopping\n",
    "    \n",
    "    # Data augmentation\n",
    "    'use_augmentation': True,\n",
    "    'augmentation_factor': 1,  # Reducido de 2 a 1 para menos datos y menos pasos (solo una ronda de augmentation)\n",
    "    \n",
    "    # Configuración para LSTM windowing\n",
    "    'window_size': 128,  # Tamaño de ventana temporal\n",
    "    'window_overlap': 0.25,  # Solapamiento entre ventanas (25% - reducido para crear menos ventanas)\n",
    "    \n",
    "    # Configuración de Google Drive\n",
    "    'google_drive_folder_id': '1aWFshMYbSlhPTZbLKldJ2Rbv7JZVYwrW',\n",
    "    'google_drive_url': 'https://drive.google.com/drive/folders/1aWFshMYbSlhPTZbLKldJ2Rbv7JZVYwrW?usp=drive_link',\n",
    "    # Nombres esperados de los archivos ZIP en Google Drive\n",
    "    'zip_left_names': ['imag_left.zip', 'left_imag.zip', 'left.zip'],\n",
    "    'zip_right_names': ['imag_right.zip', 'right_imag.zip', 'right.zip']\n",
    "}\n",
    "\n",
    "# Crear directorios necesarios\n",
    "for dir_name in [CONFIG['data_dir'], CONFIG['models_dir'], CONFIG['results_dir']]:\n",
    "    Path(dir_name).mkdir(exist_ok=True, parents=True)\n",
    "Path(CONFIG['data_dir']).joinpath('processed').mkdir(exist_ok=True, parents=True)\n",
    "Path(CONFIG['data_dir']).joinpath('left_imag').mkdir(exist_ok=True, parents=True)\n",
    "Path(CONFIG['data_dir']).joinpath('right_imag').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Configuración cargada\")\n",
    "print(f\"- Número de sujetos: {CONFIG['n_subjects']}\")\n",
    "print(f\"- Épocas pre-entrenamiento: {CONFIG['pretrain_epochs']}\")\n",
    "print(f\"- Épocas fine-tuning (por etapa): {CONFIG['finetune_epochs_stage1']}, {CONFIG['finetune_epochs_stage2']}, {CONFIG['finetune_epochs_stage3']}, {CONFIG['finetune_epochs_stage4']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones Auxiliares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones auxiliares definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones auxiliares para métricas y visualización\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calcula todas las métricas de evaluación\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'cohen_kappa': cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Matriz de Confusión\"):\n",
    "    \"\"\"Visualiza la matriz de confusión\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['MI-L', 'MI-R'], \n",
    "                yticklabels=['MI-L', 'MI-R'])\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "print(\"Funciones auxiliares definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de Datos con Rastreo de Sujetos (para LOSO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de carga de datos con sujetos definida\n"
     ]
    }
   ],
   "source": [
    "def download_data_from_drive(data_dir='data', folder_id=None, config=None):\n",
    "    \"\"\"Descarga archivos ZIP del Google Drive y los extrae en las carpetas correspondientes\"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "    if folder_id is None:\n",
    "        folder_id = config.get('google_drive_folder_id', '1aWFshMYbSlhPTZbLKldJ2Rbv7JZVYwrW')\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    left_dir = data_path / 'left_imag'\n",
    "    right_dir = data_path / 'right_imag'\n",
    "    \n",
    "    # Crear directorios si no existen\n",
    "    left_dir.mkdir(exist_ok=True, parents=True)\n",
    "    right_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Verificar si ya existen archivos\n",
    "    # Verificar si ya existen archivos (formato EEGLAB: .set y .fdt)\n",
    "    # Nota: Los archivos .fdt son binarios asociados a .set, solo contamos .set\n",
    "    left_files = list(left_dir.glob('*.set'))\n",
    "    right_files = list(right_dir.glob('*.set'))\n",
    "    \n",
    "    if len(left_files) > 0 and len(right_files) > 0:\n",
    "        print(f\"Datos encontrados: {len(left_files)} archivos en left_imag, {len(right_files)} archivos en right_imag\")\n",
    "        return\n",
    "    \n",
    "    print(\"No se encontraron datos locales. Descargando ZIPs del Google Drive...\")\n",
    "    print(f\"URL: {config.get('google_drive_url', 'N/A')}\")\n",
    "    \n",
    "    try:\n",
    "        # Descargar carpeta completa del Google Drive\n",
    "        print(\"Descargando carpeta desde Google Drive...\")\n",
    "        print(\"Esto puede tomar varios minutos...\")\n",
    "        \n",
    "        # Descargar la carpeta completa (esto descargará todos los archivos, incluyendo los ZIP)\n",
    "        temp_download_dir = data_path / 'temp_downloads'\n",
    "        temp_download_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        gdown.download_folder(\n",
    "            id=folder_id,\n",
    "            output=str(temp_download_dir),\n",
    "            quiet=False,\n",
    "            use_cookies=False\n",
    "        )\n",
    "        \n",
    "        # Buscar archivos ZIP descargados\n",
    "        zip_files = list(temp_download_dir.glob('*.zip'))\n",
    "        \n",
    "        if len(zip_files) == 0:\n",
    "            # Buscar recursivamente en subcarpetas\n",
    "            zip_files = list(temp_download_dir.rglob('*.zip'))\n",
    "        \n",
    "        print(f\"Encontrados {len(zip_files)} archivo(s) ZIP\")\n",
    "        \n",
    "        # Identificar y extraer cada ZIP\n",
    "        left_zip = None\n",
    "        right_zip = None\n",
    "        \n",
    "        # Buscar ZIPs por nombre\n",
    "        zip_left_names = config.get('zip_left_names', ['imag_left.zip', 'left_imag.zip', 'left.zip'])\n",
    "        zip_right_names = config.get('zip_right_names', ['imag_right.zip', 'right_imag.zip', 'right.zip'])\n",
    "        \n",
    "        for zip_file in zip_files:\n",
    "            zip_name_lower = zip_file.name.lower()\n",
    "            # Buscar ZIP izquierdo\n",
    "            if not left_zip:\n",
    "                for name_pattern in zip_left_names:\n",
    "                    if name_pattern.lower() in zip_name_lower or 'left' in zip_name_lower:\n",
    "                        left_zip = zip_file\n",
    "                        print(f\"Identificado ZIP izquierdo: {zip_file.name}\")\n",
    "                        break\n",
    "            # Buscar ZIP derecho\n",
    "            if not right_zip:\n",
    "                for name_pattern in zip_right_names:\n",
    "                    if name_pattern.lower() in zip_name_lower or 'right' in zip_name_lower:\n",
    "                        right_zip = zip_file\n",
    "                        print(f\"Identificado ZIP derecho: {zip_file.name}\")\n",
    "                        break\n",
    "        \n",
    "        # Si no se encontraron por nombre, intentar usar los primeros dos ZIPs\n",
    "        if not left_zip and len(zip_files) >= 2:\n",
    "            left_zip = zip_files[0]\n",
    "            right_zip = zip_files[1]\n",
    "            print(f\"Usando primeros dos ZIPs encontrados: {left_zip.name}, {right_zip.name}\")\n",
    "        elif not left_zip and len(zip_files) == 1:\n",
    "            print(f\"Solo se encontró un ZIP: {zip_files[0].name}\")\n",
    "            print(\"Asumiendo que contiene ambos conjuntos de datos...\")\n",
    "            left_zip = zip_files[0]\n",
    "        \n",
    "        # Extraer ZIPs de manera inteligente (evitar carpetas anidadas)\n",
    "        def extract_zip_smartly(zip_file, target_dir, zip_name):\n",
    "            \"\"\"Extrae un ZIP evitando crear carpetas anidadas\"\"\"\n",
    "            temp_extract = target_dir.parent / f'temp_extract_{zip_name}'\n",
    "            temp_extract.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_extract)\n",
    "            \n",
    "            # Buscar si hay una carpeta left_imag o right_imag en la raíz\n",
    "            nested_dir = None\n",
    "            for item in temp_extract.iterdir():\n",
    "                if item.is_dir() and (item.name.lower() in ['left_imag', 'right_imag', 'left', 'right']):\n",
    "                    nested_dir = item\n",
    "                    break\n",
    "            \n",
    "            # Si encontramos una carpeta anidada, mover su contenido\n",
    "            if nested_dir:\n",
    "                print(f\"Detectada carpeta '{nested_dir.name}' dentro del ZIP\")\n",
    "                # Mover todos los archivos de la carpeta anidada\n",
    "                for file in nested_dir.rglob('*'):\n",
    "                    if file.is_file():\n",
    "                        relative = file.relative_to(nested_dir)\n",
    "                        target_file = target_dir / relative\n",
    "                        target_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.move(str(file), str(target_file))\n",
    "                print(f\"Contenido movido de {nested_dir.name} a {target_dir.name}/\")\n",
    "            else:\n",
    "                # Si no hay carpeta anidada, mover todos los archivos directamente\n",
    "                for file in temp_extract.rglob('*'):\n",
    "                    if file.is_file():\n",
    "                        relative = file.relative_to(temp_extract)\n",
    "                        target_file = target_dir / relative\n",
    "                        target_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.move(str(file), str(target_file))\n",
    "            \n",
    "            # Limpiar directorio temporal\n",
    "            shutil.rmtree(temp_extract)\n",
    "        \n",
    "        if left_zip:\n",
    "            print(f\"Extrayendo {left_zip.name} a left_imag/...\")\n",
    "            extract_zip_smartly(left_zip, left_dir, 'left')\n",
    "            print(\"Extracción completada: left_imag/\")\n",
    "        \n",
    "        if right_zip:\n",
    "            print(f\"Extrayendo {right_zip.name} a right_imag/...\")\n",
    "            extract_zip_smartly(right_zip, right_dir, 'right')\n",
    "            print(\"Extracción completada: right_imag/\")\n",
    "        \n",
    "        # Limpiar archivos temporales\n",
    "        if temp_download_dir.exists():\n",
    "            shutil.rmtree(temp_download_dir)\n",
    "            print(\"Archivos temporales eliminados\")\n",
    "        \n",
    "        # Verificar archivos extraídos (formato EEGLAB: .set y .fdt)\n",
    "        # Nota: Los archivos .fdt son binarios asociados a .set, solo contamos .set\n",
    "        downloaded_left = list(left_dir.glob('*.set'))\n",
    "        downloaded_right = list(right_dir.glob('*.set'))\n",
    "        \n",
    "        if len(downloaded_left) > 0 or len(downloaded_right) > 0:\n",
    "            print(\"Descarga y extracción completada:\")\n",
    "            print(f\"- left_imag: {len(downloaded_left)} archivos .set\")\n",
    "            print(f\"- right_imag: {len(downloaded_right)} archivos .set\")\n",
    "            if len(downloaded_left) > 0:\n",
    "                fdt_files_left = list(left_dir.glob('*.fdt'))\n",
    "                if len(fdt_files_left) > 0:\n",
    "                    print(f\"- left_imag: {len(fdt_files_left)} archivos .fdt asociados\")\n",
    "            if len(downloaded_right) > 0:\n",
    "                fdt_files_right = list(right_dir.glob('*.fdt'))\n",
    "                if len(fdt_files_right) > 0:\n",
    "                    print(f\"- right_imag: {len(fdt_files_right)} archivos .fdt asociados\")\n",
    "        else:\n",
    "            print(\"Los ZIPs se extrajeron pero no se encontraron archivos .set\")\n",
    "            print(f\"Verifica la estructura de los archivos ZIP\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar o extraer: {e}\")\n",
    "        print(\"Por favor, descarga los archivos ZIP manualmente desde:\")\n",
    "        print(f\"{config.get('google_drive_url', 'N/A')}\")\n",
    "        print(\"Y extrae:\")\n",
    "        print(f\"- imag_left.zip (o similar) en: {left_dir}/\")\n",
    "        print(f\"- imag_right.zip (o similar) en: {right_dir}/\")\n",
    "        raise\n",
    "\n",
    "def load_eeg_data_with_subjects(data_dir='data', config=None):\n",
    "    \"\"\"\n",
    "    Carga datos EEG y rastrea qué trial pertenece a qué sujeto.\n",
    "    \n",
    "    Asume que los archivos están organizados de manera que podemos inferir el sujeto\n",
    "    basándonos en el número de trial (ej: 22 trials por sujeto).\n",
    "    \n",
    "    Returns:\n",
    "        X: array de trials (n_trials, n_channels, n_samples)\n",
    "        y: array de etiquetas (n_trials,)\n",
    "        subject_ids: array de IDs de sujetos (n_trials,)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    left_dir = data_path / 'left_imag'\n",
    "    right_dir = data_path / 'right_imag'\n",
    "    \n",
    "    X_all, y_all, subject_ids_all = [], [], []\n",
    "    \n",
    "    # Función auxiliar para extraer ID de sujeto del nombre del archivo\n",
    "    def extract_subject_id(file_name):\n",
    "        \"\"\"\n",
    "        Extrae el ID del sujeto del nombre del archivo.\n",
    "        Espera formato como: S001_Task2_PREP_Left.set o S001_Task2_PREP_Right.set\n",
    "        \"\"\"\n",
    "        import re\n",
    "        # Buscar patrón S seguido de números (ej: S001, S002, S15)\n",
    "        # El patrón busca 'S' seguido de 1-3 dígitos al inicio del nombre\n",
    "        match = re.search(r'S(\\d+)', str(file_name))\n",
    "        if match:\n",
    "            subject_num = int(match.group(1))\n",
    "            subject_id = subject_num - 1  # Convertir a 0-indexed (S001 -> 0, S002 -> 1)\n",
    "            return subject_id\n",
    "        # Si no encuentra el patrón S, intentar buscar cualquier número al inicio\n",
    "        match = re.search(r'^[^0-9]*(\\d+)', str(file_name))\n",
    "        if match:\n",
    "            subject_num = int(match.group(1))\n",
    "            return subject_num - 1\n",
    "        return None\n",
    "    \n",
    "    # Función auxiliar para cargar archivos y asignar sujetos\n",
    "    def load_trials_with_subjects(trial_dir, label):\n",
    "        X_trials, y_trials, subject_trials = [], [], []\n",
    "        subject_file_map = {}  # Para diagnóstico\n",
    "        \n",
    "        # Buscar archivos .set (formato EEGLAB)\n",
    "        # Nota: Los archivos .fdt se cargan automáticamente con el .set cuando se usa MNE\n",
    "        files = sorted(trial_dir.glob('*.set'))\n",
    "        \n",
    "        print(f\"Encontrados {len(files)} archivos .set\")\n",
    "        \n",
    "        for file_path in files:\n",
    "            if not file_path.is_file():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Extraer ID del sujeto del nombre del archivo\n",
    "                subject_id = extract_subject_id(file_path.name)\n",
    "                if subject_id is None:\n",
    "                    print(f\"No se pudo extraer ID de sujeto de {file_path.name}, saltando...\")\n",
    "                    continue\n",
    "                \n",
    "                # Guardar mapeo para diagnóstico\n",
    "                if subject_id not in subject_file_map:\n",
    "                    subject_file_map[subject_id] = []\n",
    "                subject_file_map[subject_id].append(file_path.name)\n",
    "                \n",
    "                # Debug: mostrar los primeros archivos procesados\n",
    "                if len(subject_file_map) <= 5 and len(subject_file_map.get(subject_id, [])) <= 2:\n",
    "                    print(f\"{file_path.name} → Sujeto ID: {subject_id}\")\n",
    "                \n",
    "                # Leer archivo EEGLAB (.set) - obtener todas las épocas\n",
    "                epochs_list = load_eeglab_file(file_path, return_all_epochs=True)\n",
    "                \n",
    "                # Iterar sobre todas las épocas del archivo\n",
    "                for epoch_data in epochs_list:\n",
    "                    # Asegurar que es un array 2D\n",
    "                    if len(epoch_data.shape) != 2:\n",
    "                        continue\n",
    "                    \n",
    "                    # Asegurar formato correcto: (channels, time)\n",
    "                    if epoch_data.shape[0] > epoch_data.shape[1]:\n",
    "                        epoch_data = epoch_data.T\n",
    "                    \n",
    "                    # Ajustar a tamaño esperado\n",
    "                    if epoch_data.shape[0] == config['n_channels'] and epoch_data.shape[1] >= config['n_samples_per_trial']:\n",
    "                        epoch_data = epoch_data[:, :config['n_samples_per_trial']]\n",
    "                    elif epoch_data.shape[1] == config['n_channels'] and epoch_data.shape[0] >= config['n_samples_per_trial']:\n",
    "                        epoch_data = epoch_data[:config['n_samples_per_trial'], :].T\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Validar tamaño final\n",
    "                    if epoch_data.shape == (config['n_channels'], config['n_samples_per_trial']):\n",
    "                        X_trials.append(epoch_data)\n",
    "                        y_trials.append(label)\n",
    "                        # Usar el ID del sujeto extraído del nombre del archivo\n",
    "                        subject_trials.append(subject_id)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error cargando {file_path.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Diagnóstico: mostrar distribución de archivos por sujeto\n",
    "        if len(subject_file_map) > 0:\n",
    "            print(\"Distribución de archivos por sujeto:\")\n",
    "            for subj_id in sorted(subject_file_map.keys()):\n",
    "                files_for_subj = subject_file_map[subj_id]\n",
    "                print(f\"- Sujeto {subj_id}: {len(files_for_subj)} archivo(s)\")\n",
    "                if len(files_for_subj) <= 3:  # Mostrar nombres si hay pocos\n",
    "                    for fname in files_for_subj:\n",
    "                        print(f\"{fname}\")\n",
    "        \n",
    "        return X_trials, y_trials, subject_trials\n",
    "    \n",
    "    print(\"Cargando datos con identificación de sujetos...\")\n",
    "    \n",
    "    # Cargar trials izquierdos\n",
    "    X_left, y_left, subjects_left = load_trials_with_subjects(left_dir, label=0)\n",
    "    unique_subjects_left = list(set(subjects_left))\n",
    "    print(f\"Left trials: {len(X_left)} trials de {len(unique_subjects_left)} sujetos únicos\")\n",
    "    print(f\"IDs de sujetos encontrados: {sorted(unique_subjects_left)}\")\n",
    "    \n",
    "    # Cargar trials derechos\n",
    "    X_right, y_right, subjects_right = load_trials_with_subjects(right_dir, label=1)\n",
    "    unique_subjects_right = list(set(subjects_right))\n",
    "    print(f\"Right trials: {len(X_right)} trials de {len(unique_subjects_right)} sujetos únicos\")\n",
    "    print(f\"IDs de sujetos encontrados: {sorted(unique_subjects_right)}\")\n",
    "    \n",
    "    # Si no hay datos, intentar descargarlos\n",
    "    if len(X_left) == 0 or len(X_right) == 0:\n",
    "        print(f\"No se encontraron datos en las carpetas. Intentando descargar del Google Drive...\")\n",
    "        download_data_from_drive(data_dir, config.get('google_drive_folder_id'), config)\n",
    "        \n",
    "        # Intentar cargar nuevamente después de la descarga\n",
    "        X_left, y_left, subjects_left = load_trials_with_subjects(left_dir, label=0)\n",
    "        X_right, y_right, subjects_right = load_trials_with_subjects(right_dir, label=1)\n",
    "    \n",
    "    # Validar que ahora sí se encontraron datos reales\n",
    "    if len(X_left) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            f\"ERROR: No se pudieron descargar ni encontrar datos reales en {left_dir}\\n\"\n",
    "            f\"   Por favor, descarga los datos manualmente desde:\\n\"\n",
    "            f\"   {config.get('google_drive_url', 'https://drive.google.com/drive/folders/1aWFshMYbSlhPTZbLKldJ2Rbv7JZVYwrW')}\\n\"\n",
    "            f\"   Y colócalos en: {left_dir}/\"\n",
    "        )\n",
    "    \n",
    "    if len(X_right) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            f\"ERROR: No se pudieron descargar ni encontrar datos reales en {right_dir}\\n\"\n",
    "            f\"   Por favor, descarga los datos manualmente desde:\\n\"\n",
    "            f\"   {config.get('google_drive_url', 'https://drive.google.com/drive/folders/1aWFshMYbSlhPTZbLKldJ2Rbv7JZVYwrW')}\\n\"\n",
    "            f\"   Y colócalos en: {right_dir}/\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Datos reales encontrados: {len(X_left)} trials izquierdos, {len(X_right)} trials derechos\")\n",
    "    \n",
    "    # Combinar left y right\n",
    "    X_all = X_left + X_right\n",
    "    y_all = y_left + y_right\n",
    "    subject_ids_all = subjects_left + subjects_right\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    X = np.array(X_all)\n",
    "    y = np.array(y_all)\n",
    "    subject_ids = np.array(subject_ids_all)\n",
    "    \n",
    "    # Asegurar que los subject_ids estén en rango [0, n_subjects-1]\n",
    "    unique_subjects = np.unique(subject_ids)\n",
    "    print(f\"\\nAntes del mapeo: {len(unique_subjects)} sujetos únicos con IDs: {sorted(unique_subjects)}\")\n",
    "    \n",
    "    # Crear mapeo para normalizar IDs a [0, n_subjects-1]\n",
    "    subject_id_mapping = {old_id: new_id for new_id, old_id in enumerate(sorted(unique_subjects))}\n",
    "    print(f\"Mapeo de IDs: {subject_id_mapping}\")\n",
    "    \n",
    "    subject_ids = np.array([subject_id_mapping[old_id] for old_id in subject_ids])\n",
    "    \n",
    "    unique_subjects_final = np.unique(subject_ids)\n",
    "    print(f\"\\nDatos cargados exitosamente:\")\n",
    "    print(f\"- Shape de X: {X.shape}\")\n",
    "    print(f\"- Número de trials: {len(y)}\")\n",
    "    print(f\"- Sujetos únicos: {len(unique_subjects_final)}\")\n",
    "    print(f\"- IDs de sujetos (final): {sorted(unique_subjects_final)}\")\n",
    "    print(f\"- Distribución por sujeto: {np.bincount(subject_ids)}\")\n",
    "    print(f\"- Distribución por clase: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, subject_ids\n",
    "\n",
    "print(\"Función de carga de datos con sujetos definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de data augmentation definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones de data augmentation para señales EEG\n",
    "def add_gaussian_noise(trial, noise_factor=0.1):\n",
    "    \"\"\"Añade ruido gaussiano a la señal EEG\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, trial.shape)\n",
    "    return trial + noise\n",
    "\n",
    "def time_warp(trial, sigma=0.2, knot=4):\n",
    "    \"\"\"Aplica time warping (estiramiento/compresión temporal)\"\"\"\n",
    "    orig_steps = np.arange(trial.shape[-1])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2,))\n",
    "    warp_steps = (np.ones((knot + 2,)) * (trial.shape[-1] - 1) / (knot + 1)).cumsum()\n",
    "    warp_steps[0] = 0\n",
    "    warp_steps[-1] = trial.shape[-1] - 1\n",
    "    warp_steps_new = warp_steps * random_warps\n",
    "    warp_steps_new = np.clip(warp_steps_new, 0, trial.shape[-1] - 1)\n",
    "    warped_trial = np.zeros_like(trial)\n",
    "    for channel in range(trial.shape[0]):\n",
    "        try:\n",
    "            f = interpolate.interp1d(warp_steps_new, trial[channel, :][warp_steps_new.astype(int)],\n",
    "                                     kind='cubic', fill_value='extrapolate', bounds_error=False)\n",
    "            warped_trial[channel, :] = f(orig_steps)\n",
    "        except:\n",
    "            f = interpolate.interp1d(warp_steps_new, trial[channel, :][warp_steps_new.astype(int)],\n",
    "                                     kind='linear', fill_value='extrapolate', bounds_error=False)\n",
    "            warped_trial[channel, :] = f(orig_steps)\n",
    "    return warped_trial\n",
    "\n",
    "def amplitude_scale(trial, min_scale=0.8, max_scale=1.2):\n",
    "    \"\"\"Escala la amplitud de la señal\"\"\"\n",
    "    scale_factor = np.random.uniform(min_scale, max_scale)\n",
    "    return trial * scale_factor\n",
    "\n",
    "def time_shift(trial, max_shift=None):\n",
    "    \"\"\"Desplaza la señal temporalmente\"\"\"\n",
    "    if max_shift is None:\n",
    "        max_shift = trial.shape[-1] // 10\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    shifted_trial = np.roll(trial, shift, axis=-1)\n",
    "    return shifted_trial\n",
    "\n",
    "def channel_dropout(trial, drop_prob=0.1):\n",
    "    \"\"\"Elimina aleatoriamente algunos canales (pone en cero)\"\"\"\n",
    "    augmented = trial.copy()\n",
    "    n_channels_to_drop = int(trial.shape[0] * drop_prob)\n",
    "    if n_channels_to_drop > 0:\n",
    "        channels_to_drop = np.random.choice(\n",
    "            trial.shape[0],\n",
    "            size=n_channels_to_drop,\n",
    "            replace=False\n",
    "        )\n",
    "        augmented[channels_to_drop, :] = 0\n",
    "    return augmented\n",
    "\n",
    "def augment_eeg_trial(trial, augmentation_methods=['gaussian_noise', 'amplitude_scale']):\n",
    "    \"\"\"Aplica una técnica de aumento aleatoria a un trial\"\"\"\n",
    "    method = np.random.choice(augmentation_methods)\n",
    "    if method == 'gaussian_noise':\n",
    "        return add_gaussian_noise(trial, noise_factor=0.1)\n",
    "    elif method == 'time_warp':\n",
    "        return time_warp(trial, sigma=0.2)\n",
    "    elif method == 'amplitude_scale':\n",
    "        return amplitude_scale(trial, min_scale=0.8, max_scale=1.2)\n",
    "    elif method == 'time_shift':\n",
    "        return time_shift(trial, max_shift=trial.shape[-1] // 10)\n",
    "    elif method == 'channel_dropout':\n",
    "        return channel_dropout(trial, drop_prob=0.1)\n",
    "    else:\n",
    "        return trial\n",
    "\n",
    "def apply_data_augmentation(X, y, subject_ids=None, augmentation_factor=2, augmentation_methods=None):\n",
    "    \"\"\"Aplica data augmentation a los datos de entrenamiento\"\"\"\n",
    "    if augmentation_methods is None:\n",
    "        augmentation_methods = ['gaussian_noise', 'amplitude_scale', 'time_warp', 'time_shift']\n",
    "    \n",
    "    # Asegurar que augmentation_factor sea un entero\n",
    "    augmentation_factor = int(augmentation_factor)\n",
    "    \n",
    "    print(f\"Aplicando data augmentation (factor: {augmentation_factor}x)...\")\n",
    "    print(f\"Métodos: {', '.join(augmentation_methods)}\")\n",
    "    \n",
    "    X_augmented = [X]\n",
    "    y_augmented = [y]\n",
    "    subject_ids_augmented = [subject_ids] if subject_ids is not None else None\n",
    "    \n",
    "    for i in range(augmentation_factor):\n",
    "        X_aug = np.array([augment_eeg_trial(trial, augmentation_methods) for trial in X])\n",
    "        X_augmented.append(X_aug)\n",
    "        y_augmented.append(y)\n",
    "        if subject_ids is not None:\n",
    "            subject_ids_augmented.append(subject_ids)\n",
    "    \n",
    "    X_final = np.concatenate(X_augmented, axis=0)\n",
    "    y_final = np.concatenate(y_augmented, axis=0)\n",
    "    \n",
    "    if subject_ids is not None:\n",
    "        subject_ids_final = np.concatenate(subject_ids_augmented, axis=0)\n",
    "    else:\n",
    "        subject_ids_final = None\n",
    "    \n",
    "    print(f\"Original: {X.shape[0]} muestras\")\n",
    "    print(f\"Aumentado: {X_final.shape[0]} muestras\")\n",
    "    print(f\"Factor: {X_final.shape[0] / X.shape[0]:.1f}x\")\n",
    "    \n",
    "    if subject_ids is not None:\n",
    "        return X_final, y_final, subject_ids_final\n",
    "    else:\n",
    "        return X_final, y_final\n",
    "\n",
    "print(\"Funciones de data augmentation definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de preparación de datos para LSTM definidas\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_lstm(X, window_size=128, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Convierte los trials EEG en ventanas temporales para LSTM.\n",
    "    \n",
    "    Args:\n",
    "        X: Array de forma (n_trials, n_channels, n_samples)\n",
    "        window_size: Tamaño de la ventana temporal\n",
    "        overlap: Solapamiento entre ventanas (0.0-1.0)\n",
    "    \n",
    "    Returns:\n",
    "        X_windows: Array de forma (n_windows, window_size, n_channels)\n",
    "        window_labels: Array con el índice del trial original para cada ventana\n",
    "    \"\"\"\n",
    "    n_trials, n_channels, n_samples = X.shape\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    \n",
    "    X_windows = []\n",
    "    window_labels = []\n",
    "    \n",
    "    for trial_idx in range(n_trials):\n",
    "        trial = X[trial_idx]  # (n_channels, n_samples)\n",
    "        \n",
    "        # Crear ventanas superpuestas\n",
    "        start = 0\n",
    "        while start + window_size <= n_samples:\n",
    "            window = trial[:, start:start + window_size]  # (n_channels, window_size)\n",
    "            # Transponer para LSTM: (window_size, n_channels)\n",
    "            window = window.T\n",
    "            X_windows.append(window)\n",
    "            window_labels.append(trial_idx)\n",
    "            start += step_size\n",
    "    \n",
    "    return np.array(X_windows), np.array(window_labels)\n",
    "\n",
    "def prepare_labels_for_lstm(y, window_labels):\n",
    "    \"\"\"\n",
    "    Asigna las etiquetas a las ventanas basándose en el trial original.\n",
    "    \n",
    "    Args:\n",
    "        y: Etiquetas originales por trial (n_trials,)\n",
    "        window_labels: Índices del trial original para cada ventana\n",
    "    \n",
    "    Returns:\n",
    "        y_windows: Etiquetas para cada ventana (n_windows,)\n",
    "    \"\"\"\n",
    "    return y[window_labels]\n",
    "\n",
    "print(\"Funciones de preparación de datos para LSTM definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Arquitectura CNN-LSTM Mejorada con Nombres de Capas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de creación del modelo definida\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_lstm_model(input_shape, initial_lr=0.001):\n",
    "    \"\"\"\n",
    "    Crea el modelo CNN-LSTM mejorado.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: (window_size, n_channels)\n",
    "        initial_lr: learning rate inicial\n",
    "    \n",
    "    Returns:\n",
    "        model: modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Capas convolucionales 1D para extraer características espaciales\n",
    "        Conv1D(64, 3, activation='relu',\n",
    "               input_shape=input_shape,\n",
    "               kernel_regularizer=l2(1e-4), name='conv1d_1'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_1'),\n",
    "        MaxPooling1D(2, name='pool_1'),\n",
    "        Dropout(0.25, name='dropout_1'),\n",
    "        \n",
    "        Conv1D(128, 3, activation='relu',\n",
    "               kernel_regularizer=l2(1e-4), name='conv1d_2'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_2'),\n",
    "        MaxPooling1D(2, name='pool_2'),\n",
    "        Dropout(0.25, name='dropout_2'),\n",
    "        \n",
    "        Conv1D(256, 3, activation='relu',\n",
    "               kernel_regularizer=l2(1e-4), name='conv1d_3'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_3'),\n",
    "        MaxPooling1D(2, name='pool_3'),\n",
    "        Dropout(0.25, name='dropout_3'),\n",
    "        \n",
    "        # Bidirectional LSTM para modelar dependencias temporales\n",
    "        Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3), name='lstm_1'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_4'),\n",
    "        \n",
    "        Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3), name='lstm_2'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_5'),\n",
    "        \n",
    "        # Capas fully connected\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(1e-4), name='dense_1'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_6'),\n",
    "        Dropout(0.5, name='dropout_4'),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(1e-4), name='dense_2'),\n",
    "        BatchNormalization(momentum=0.99, epsilon=1e-3, name='bn_7'),\n",
    "        Dropout(0.5, name='dropout_5'),\n",
    "        Dense(2, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=initial_lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Función de creación del modelo definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-entrenamiento Multi-Sujeto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de pre-entrenamiento definida\n"
     ]
    }
   ],
   "source": [
    "def pre_train_multisubject(model, X_train, y_train, config, save_path=None):\n",
    "    \"\"\"\n",
    "    Pre-entrena el modelo con datos de múltiples sujetos.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo CNN-LSTM a pre-entrenar\n",
    "        X_train: Datos de entrenamiento (n_windows, window_size, n_channels)\n",
    "        y_train: Etiquetas (n_windows,)\n",
    "        config: Diccionario de configuración\n",
    "        save_path: Ruta opcional para guardar el modelo pre-entrenado\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo pre-entrenado\n",
    "        history: Historial de entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FASE 1: PRE-ENTRENAMIENTO MULTI-SUJETO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Convertir etiquetas a categóricas\n",
    "    y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "    \n",
    "    # Calcular class weights para balancear clases\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Callbacks optimizados\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=config.get('early_stopping_patience', 5),\n",
    "            restore_best_weights=True,\n",
    "            min_delta=config.get('pretrain_min_delta', 1e-5),\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Más agresivo: reduce LR después de 3 épocas sin mejora\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    if save_path:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                save_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(f\"Entrenando modelo con {len(X_train)} ventanas de {len(np.unique(y_train))} sujetos...\")\n",
    "    print(f\"Learning rate inicial: {config['pretrain_lr']}\")\n",
    "    print(f\"Épocas máximas: {config['pretrain_epochs']}\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['pretrain_epochs'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Calcular épocas realmente ejecutadas\n",
    "    actual_epochs = len(history.history['loss'])\n",
    "    max_epochs = config['pretrain_epochs']\n",
    "    \n",
    "    print(f\"\\nPre-entrenamiento completado:\")\n",
    "    print(f\"- Épocas ejecutadas: {actual_epochs}/{max_epochs}\")\n",
    "    print(f\"- Mejor val_loss: {min(history.history['val_loss']):.6f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Función de pre-entrenamiento definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de fine-tuning progresivo definida\n"
     ]
    }
   ],
   "source": [
    "def progressive_fine_tuning(model, X_train, y_train, config, verbose=True):\n",
    "    \"\"\"\n",
    "    Aplica fine-tuning progresivo en 4 etapas.\n",
    "    \n",
    "    Etapa 1: Congelar CNN, fine-tune solo capas Dense\n",
    "    Etapa 2: Descongelar LSTM + Dense, fine-tune\n",
    "    Etapa 3: Descongelar últimas capas CNN + LSTM + Dense, fine-tune\n",
    "    Etapa 4: Fine-tune completo con learning rate muy bajo\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo pre-entrenado\n",
    "        X_train: Datos de entrenamiento\n",
    "        y_train: Etiquetas\n",
    "        config: Diccionario de configuración\n",
    "        verbose: Si mostrar progreso detallado\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo fine-tuneado\n",
    "        all_history: Lista de historiales de todas las etapas\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FASE 2: FINE-TUNING PROGRESIVO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "    \n",
    "    # Calcular class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    all_history = []\n",
    "    \n",
    "    # ETAPA 1: Solo capas Dense\n",
    "    if verbose:\n",
    "        print(\"\\n--- Etapa 1: Fine-tuning de capas Dense ---\")\n",
    "    \n",
    "    # Congelar todas las capas excepto las Dense\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        if layer.name.startswith('dense'):\n",
    "            layer.trainable = True\n",
    "    \n",
    "    # Recompilar con learning rate más bajo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['finetune_lr_stage1']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    callbacks_stage1 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=config.get('finetune_early_stopping_patience', 2),\n",
    "            restore_best_weights=True,\n",
    "            min_delta=config.get('finetune_min_delta', 1e-4),\n",
    "            verbose=1 if verbose else 0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=1,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1 if verbose else 0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history1 = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['finetune_epochs_stage1'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks_stage1,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    all_history.append(history1)\n",
    "    \n",
    "    # ETAPA 2: LSTM + Dense\n",
    "    if verbose:\n",
    "        print(\"\\n--- Etapa 2: Fine-tuning de LSTM + Dense ---\")\n",
    "    \n",
    "    # Descongelar LSTM y Dense\n",
    "    for layer in model.layers:\n",
    "        if 'lstm' in layer.name or layer.name.startswith('dense') or 'bn_4' in layer.name or 'bn_5' in layer.name:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['finetune_lr_stage2']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    callbacks_stage2 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=config.get('finetune_early_stopping_patience', 2),\n",
    "            restore_best_weights=True,\n",
    "            min_delta=config.get('finetune_min_delta', 1e-4),\n",
    "            verbose=1 if verbose else 0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=1,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1 if verbose else 0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history2 = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['finetune_epochs_stage2'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks_stage2,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    all_history.append(history2)\n",
    "    \n",
    "    # ETAPA 3: Última capa CNN + LSTM + Dense\n",
    "    if verbose:\n",
    "        print(\"\\n--- Etapa 3: Fine-tuning de CNN final + LSTM + Dense ---\")\n",
    "    \n",
    "    # Descongelar última capa CNN (conv1d_3) y todo lo demás\n",
    "    for layer in model.layers:\n",
    "        if 'conv1d_3' in layer.name or 'pool_3' in layer.name or 'bn_3' in layer.name or 'dropout_3' in layer.name:\n",
    "            layer.trainable = True\n",
    "        if 'lstm' in layer.name or layer.name.startswith('dense'):\n",
    "            layer.trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['finetune_lr_stage3']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    callbacks_stage3 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=config.get('finetune_early_stopping_patience', 2),\n",
    "            restore_best_weights=True,\n",
    "            min_delta=config.get('finetune_min_delta', 1e-4),\n",
    "            verbose=1 if verbose else 0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=1,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1 if verbose else 0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history3 = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['finetune_epochs_stage3'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks_stage3,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    all_history.append(history3)\n",
    "    \n",
    "    # ETAPA 4: Fine-tune completo\n",
    "    if verbose:\n",
    "        print(\"\\n--- Etapa 4: Fine-tuning completo (LR muy bajo) ---\")\n",
    "    \n",
    "    # Descongelar todas las capas\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['finetune_lr_stage4']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    callbacks_stage4 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=config.get('finetune_early_stopping_patience', 2),\n",
    "            restore_best_weights=True,\n",
    "            min_delta=config.get('finetune_min_delta', 1e-4),\n",
    "            verbose=1 if verbose else 0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=1,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1 if verbose else 0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history4 = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['finetune_epochs_stage4'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks_stage4,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    all_history.append(history4)\n",
    "    \n",
    "    # Calcular tiempo total de fine-tuning\n",
    "    total_epochs = (\n",
    "        len(history1.history['loss']) +\n",
    "        len(history2.history['loss']) +\n",
    "        len(history3.history['loss']) +\n",
    "        len(history4.history['loss'])\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFine-tuning progresivo completado:\")\n",
    "        print(f\"- Épocas totales ejecutadas: {total_epochs}\")\n",
    "        print(f\"- Épocas máximas configuradas: {config['finetune_epochs_stage1'] + config['finetune_epochs_stage2'] + config['finetune_epochs_stage3'] + config['finetune_epochs_stage4']}\")\n",
    "    \n",
    "    return model, all_history\n",
    "\n",
    "print(\"Función de fine-tuning progresivo definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de Leave-One-Subject-Out CV definida\n"
     ]
    }
   ],
   "source": [
    "def leave_one_subject_out_cv(X, y, subject_ids, config, use_augmentation=True):\n",
    "    \"\"\"\n",
    "    Implementa Leave-One-Subject-Out cross-validation con transfer learning.\n",
    "    \n",
    "    Para cada sujeto:\n",
    "    1. Pre-entrenar con datos de los otros N-1 sujetos\n",
    "    2. Fine-tune con datos del sujeto objetivo\n",
    "    3. Evaluar en el sujeto de prueba\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FASE 3: LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    unique_subjects = np.unique(subject_ids)\n",
    "    n_subjects = len(unique_subjects)\n",
    "    \n",
    "    print(f\"\\nIniciando LOSO CV con {n_subjects} sujetos...\")\n",
    "    print(f\"Total de iteraciones: {n_subjects}\")\n",
    "    print(f\"Este proceso puede tomar varias horas...\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    subject_results = {}\n",
    "    \n",
    "    # Usar LeaveOneGroupOut de sklearn\n",
    "    logo = LeaveOneGroupOut()\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y, groups=subject_ids)):\n",
    "        test_subject_id = subject_ids[test_idx[0]]\n",
    "        train_subjects = np.unique(subject_ids[train_idx])\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ITERACIÓN {fold_idx + 1}/{n_subjects}: Sujeto {test_subject_id} como prueba\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"- Sujetos de entrenamiento: {sorted(train_subjects)}\")\n",
    "        print(f\"- Sujeto de prueba: {test_subject_id}\")\n",
    "        print(f\"- Trials entrenamiento: {len(train_idx)}\")\n",
    "        print(f\"- Trials prueba: {len(test_idx)}\")\n",
    "        \n",
    "        # Separar datos\n",
    "        X_train_fold = X[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        subject_ids_train_fold = subject_ids[train_idx]\n",
    "        \n",
    "        X_test_fold = X[test_idx]\n",
    "        y_test_fold = y[test_idx]\n",
    "        \n",
    "        # Aplicar data augmentation si está habilitado\n",
    "        if use_augmentation and config.get('use_augmentation', False):\n",
    "            X_train_fold, y_train_fold, subject_ids_train_fold = apply_data_augmentation(\n",
    "                X_train_fold, y_train_fold, subject_ids_train_fold,\n",
    "                augmentation_factor=config.get('augmentation_factor', 2),\n",
    "                augmentation_methods=['gaussian_noise', 'amplitude_scale', 'time_warp', 'time_shift']\n",
    "            )\n",
    "        \n",
    "        # Preparar datos para LSTM\n",
    "        X_train_windows, window_labels_train = prepare_data_for_lstm(\n",
    "            X_train_fold, \n",
    "            window_size=config['window_size'],\n",
    "            overlap=config['window_overlap']\n",
    "        )\n",
    "        y_train_windows = prepare_labels_for_lstm(y_train_fold, window_labels_train)\n",
    "        \n",
    "        X_test_windows, window_labels_test = prepare_data_for_lstm(\n",
    "            X_test_fold,\n",
    "            window_size=config['window_size'],\n",
    "            overlap=config['window_overlap']\n",
    "        )\n",
    "        y_test_windows = prepare_labels_for_lstm(y_test_fold, window_labels_test)\n",
    "        \n",
    "        # Crear nuevo modelo para esta iteración\n",
    "        model = create_cnn_lstm_model(\n",
    "            input_shape=(X_train_windows.shape[1], X_train_windows.shape[2]),\n",
    "            initial_lr=config['pretrain_lr']\n",
    "        )\n",
    "        \n",
    "        # FASE 1: Pre-entrenamiento con múltiples sujetos\n",
    "        pretrain_start = time.time()\n",
    "        model, pretrain_history = pre_train_multisubject(\n",
    "            model, X_train_windows, y_train_windows, config, save_path=None\n",
    "        )\n",
    "        pretrain_time = time.time() - pretrain_start\n",
    "        \n",
    "        # FASE 2: Fine-tuning progresivo con datos del sujeto objetivo\n",
    "        finetune_start = time.time()\n",
    "        model, finetune_history = progressive_fine_tuning(\n",
    "            model, X_train_windows, y_train_windows, config, verbose=True\n",
    "        )\n",
    "        finetune_time = time.time() - finetune_start\n",
    "        \n",
    "        # Evaluar en el sujeto de prueba\n",
    "        y_test_cat = to_categorical(y_test_windows, num_classes=2)\n",
    "        y_pred_proba = model.predict(X_test_windows, verbose=0, batch_size=32)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        metrics = calculate_metrics(y_test_windows, y_pred)\n",
    "        metrics['subject_id'] = int(test_subject_id)\n",
    "        metrics['pretrain_time'] = pretrain_time / 60  # en minutos\n",
    "        metrics['finetune_time'] = finetune_time / 60  # en minutos\n",
    "        metrics['total_time'] = (pretrain_time + finetune_time) / 60  # en minutos\n",
    "        \n",
    "        all_results.append(metrics)\n",
    "        subject_results[int(test_subject_id)] = metrics\n",
    "        \n",
    "        print(f\"\\nResultados para sujeto {test_subject_id}:\")\n",
    "        print(f\"- Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"- F1-score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"- Cohen's κ: {metrics['cohen_kappa']:.4f}\")\n",
    "        print(f\"- Tiempo total: {metrics['total_time']:.2f} minutos\")\n",
    "        \n",
    "        # Limpiar memoria\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Calcular métricas promedio\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in all_results]),\n",
    "        'precision': np.mean([r['precision'] for r in all_results]),\n",
    "        'recall': np.mean([r['recall'] for r in all_results]),\n",
    "        'f1_score': np.mean([r['f1_score'] for r in all_results]),\n",
    "        'cohen_kappa': np.mean([r['cohen_kappa'] for r in all_results]),\n",
    "        'std_accuracy': np.std([r['accuracy'] for r in all_results]),\n",
    "        'std_f1': np.std([r['f1_score'] for r in all_results]),\n",
    "        'total_time': np.sum([r['total_time'] for r in all_results])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESULTADOS FINALES - LEAVE-ONE-SUBJECT-OUT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nMétricas Promedio:\")\n",
    "    print(f\"- Accuracy: {avg_metrics['accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n",
    "    print(f\"- Precision: {avg_metrics['precision']:.4f}\")\n",
    "    print(f\"- Recall: {avg_metrics['recall']:.4f}\")\n",
    "    print(f\"- F1-score: {avg_metrics['f1_score']:.4f} ± {avg_metrics['std_f1']:.4f}\")\n",
    "    print(f\"- Cohen's κ: {avg_metrics['cohen_kappa']:.4f}\")\n",
    "    print(f\"\\nTiempo total: {avg_metrics['total_time']:.2f} minutos ({avg_metrics['total_time']/60:.2f} horas)\")\n",
    "    \n",
    "    return all_results, subject_results, avg_metrics\n",
    "\n",
    "print(\"Función de Leave-One-Subject-Out CV definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CARGANDO DATOS CON RASTREO DE SUJETOS\n",
      "======================================================================\n",
      "Cargando datos con identificación de sujetos...\n",
      "Encontrados 20 archivos .set\n",
      "S001_Task2_PREP_Left.set → Sujeto ID: 0\n",
      "S002_Task2_PREP_Left.set → Sujeto ID: 1\n",
      "S003_Task2_PREP_Left.set → Sujeto ID: 2\n",
      "S004_Task2_PREP_Left.set → Sujeto ID: 3\n",
      "S005_Task2_PREP_Left.set → Sujeto ID: 4\n",
      "Distribución de archivos por sujeto:\n",
      "- Sujeto 0: 1 archivo(s)\n",
      "S001_Task2_PREP_Left.set\n",
      "- Sujeto 1: 1 archivo(s)\n",
      "S002_Task2_PREP_Left.set\n",
      "- Sujeto 2: 1 archivo(s)\n",
      "S003_Task2_PREP_Left.set\n",
      "- Sujeto 3: 1 archivo(s)\n",
      "S004_Task2_PREP_Left.set\n",
      "- Sujeto 4: 1 archivo(s)\n",
      "S005_Task2_PREP_Left.set\n",
      "- Sujeto 5: 1 archivo(s)\n",
      "S006_Task2_PREP_Left.set\n",
      "- Sujeto 6: 1 archivo(s)\n",
      "S007_Task2_PREP_Left.set\n",
      "- Sujeto 7: 1 archivo(s)\n",
      "S008_Task2_PREP_Left.set\n",
      "- Sujeto 8: 1 archivo(s)\n",
      "S009_Task2_PREP_Left.set\n",
      "- Sujeto 9: 1 archivo(s)\n",
      "S010_Task2_PREP_Left.set\n",
      "- Sujeto 10: 1 archivo(s)\n",
      "S011_Task2_PREP_Left.set\n",
      "- Sujeto 11: 1 archivo(s)\n",
      "S012_Task2_PREP_Left.set\n",
      "- Sujeto 12: 1 archivo(s)\n",
      "S013_Task2_PREP_Left.set\n",
      "- Sujeto 13: 1 archivo(s)\n",
      "S014_Task2_PREP_Left.set\n",
      "- Sujeto 14: 1 archivo(s)\n",
      "S015_Task2_PREP_Left.set\n",
      "- Sujeto 15: 1 archivo(s)\n",
      "S016_Task2_PREP_Left.set\n",
      "- Sujeto 16: 1 archivo(s)\n",
      "S017_Task2_PREP_Left.set\n",
      "- Sujeto 17: 1 archivo(s)\n",
      "S018_Task2_PREP_Left.set\n",
      "- Sujeto 18: 1 archivo(s)\n",
      "S019_Task2_PREP_Left.set\n",
      "- Sujeto 19: 1 archivo(s)\n",
      "S020_Task2_PREP_Left.set\n",
      "Left trials: 442 trials de 20 sujetos únicos\n",
      "IDs de sujetos encontrados: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Encontrados 20 archivos .set\n",
      "S001_Task2_PREP_Right.set → Sujeto ID: 0\n",
      "S002_Task2_PREP_Right.set → Sujeto ID: 1\n",
      "S003_Task2_PREP_Right.set → Sujeto ID: 2\n",
      "S004_Task2_PREP_Right.set → Sujeto ID: 3\n",
      "S005_Task2_PREP_Right.set → Sujeto ID: 4\n",
      "Distribución de archivos por sujeto:\n",
      "- Sujeto 0: 1 archivo(s)\n",
      "S001_Task2_PREP_Right.set\n",
      "- Sujeto 1: 1 archivo(s)\n",
      "S002_Task2_PREP_Right.set\n",
      "- Sujeto 2: 1 archivo(s)\n",
      "S003_Task2_PREP_Right.set\n",
      "- Sujeto 3: 1 archivo(s)\n",
      "S004_Task2_PREP_Right.set\n",
      "- Sujeto 4: 1 archivo(s)\n",
      "S005_Task2_PREP_Right.set\n",
      "- Sujeto 5: 1 archivo(s)\n",
      "S006_Task2_PREP_Right.set\n",
      "- Sujeto 6: 1 archivo(s)\n",
      "S007_Task2_PREP_Right.set\n",
      "- Sujeto 7: 1 archivo(s)\n",
      "S008_Task2_PREP_Right.set\n",
      "- Sujeto 8: 1 archivo(s)\n",
      "S009_Task2_PREP_Right.set\n",
      "- Sujeto 9: 1 archivo(s)\n",
      "S010_Task2_PREP_Right.set\n",
      "- Sujeto 10: 1 archivo(s)\n",
      "S011_Task2_PREP_Right.set\n",
      "- Sujeto 11: 1 archivo(s)\n",
      "S012_Task2_PREP_Right.set\n",
      "- Sujeto 12: 1 archivo(s)\n",
      "S013_Task2_PREP_Right.set\n",
      "- Sujeto 13: 1 archivo(s)\n",
      "S014_Task2_PREP_Right.set\n",
      "- Sujeto 14: 1 archivo(s)\n",
      "S015_Task2_PREP_Right.set\n",
      "- Sujeto 15: 1 archivo(s)\n",
      "S016_Task2_PREP_Right.set\n",
      "- Sujeto 16: 1 archivo(s)\n",
      "S017_Task2_PREP_Right.set\n",
      "- Sujeto 17: 1 archivo(s)\n",
      "S018_Task2_PREP_Right.set\n",
      "- Sujeto 18: 1 archivo(s)\n",
      "S019_Task2_PREP_Right.set\n",
      "- Sujeto 19: 1 archivo(s)\n",
      "S020_Task2_PREP_Right.set\n",
      "Right trials: 438 trials de 20 sujetos únicos\n",
      "IDs de sujetos encontrados: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Datos reales encontrados: 442 trials izquierdos, 438 trials derechos\n",
      "\n",
      "Antes del mapeo: 20 sujetos únicos con IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Mapeo de IDs: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}\n",
      "\n",
      "Datos cargados exitosamente:\n",
      "- Shape de X: (880, 64, 1152)\n",
      "- Número de trials: 880\n",
      "- Sujetos únicos: 20\n",
      "- IDs de sujetos (final): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "- Distribución por sujeto: [44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44]\n",
      "- Distribución por clase: [442 438]\n",
      "\n",
      "Datos cargados (antes del filtrado):\n",
      "- Shape X: (880, 64, 1152)\n",
      "- Shape y: (880,)\n",
      "- Shape subject_ids: (880,)\n",
      "- Sujetos únicos: 20\n",
      "\n",
      "======================================================================\n",
      "MODO PRUEBA: Procesando solo 2 sujetos\n",
      "======================================================================\n",
      "Sujetos seleccionados: [0, 1]\n",
      "Para ejecución completa, cambiar MAX_SUBJECTS = None\n",
      "\n",
      "Datos cargados (después del filtrado):\n",
      "- Shape X: (88, 64, 1152)\n",
      "- Shape y: (88,)\n",
      "- Shape subject_ids: (88,)\n",
      "- Sujetos únicos: 2\n",
      "\n",
      "======================================================================\n",
      "INICIANDO LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION\n",
      "======================================================================\n",
      "MODO PRUEBA: Procesando 2 sujetos (~10-30 minutos)\n",
      "Total de iteraciones: 2\n",
      "Cada iteración incluye:\n",
      "- Pre-entrenamiento con múltiples sujetos\n",
      "- Fine-tuning progresivo (4 etapas)\n",
      "- Evaluación\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FASE 3: LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Iniciando LOSO CV con 2 sujetos...\n",
      "Total de iteraciones: 2\n",
      "Este proceso puede tomar varias horas...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ITERACIÓN 1/2: Sujeto 0 como prueba\n",
      "======================================================================\n",
      "- Sujetos de entrenamiento: [1]\n",
      "- Sujeto de prueba: 0\n",
      "- Trials entrenamiento: 44\n",
      "- Trials prueba: 44\n",
      "Aplicando data augmentation (factor: 1x)...\n",
      "Métodos: gaussian_noise, amplitude_scale, time_warp, time_shift\n",
      "Original: 44 muestras\n",
      "Aumentado: 88 muestras\n",
      "Factor: 2.0x\n",
      "\n",
      "======================================================================\n",
      "FASE 1: PRE-ENTRENAMIENTO MULTI-SUJETO\n",
      "======================================================================\n",
      "Entrenando modelo con 968 ventanas de 2 sujetos...\n",
      "Learning rate inicial: 0.001\n",
      "Épocas máximas: 30\n",
      "Epoch 1/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 983ms/step - accuracy: 0.4976 - loss: 1.2713 - val_accuracy: 0.0000e+00 - val_loss: 0.7553 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 267ms/step - accuracy: 0.5231 - loss: 1.1783 - val_accuracy: 0.0000e+00 - val_loss: 0.7591 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 254ms/step - accuracy: 0.4903 - loss: 1.2009 - val_accuracy: 0.0000e+00 - val_loss: 0.7808 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.5130 - loss: 1.1035\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - accuracy: 0.5195 - loss: 1.0693 - val_accuracy: 0.0000e+00 - val_loss: 0.8136 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 247ms/step - accuracy: 0.5292 - loss: 1.0413 - val_accuracy: 0.0000e+00 - val_loss: 0.8071 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 247ms/step - accuracy: 0.5207 - loss: 1.0640 - val_accuracy: 0.0000e+00 - val_loss: 0.8055 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Pre-entrenamiento completado:\n",
      "- Épocas ejecutadas: 6/30\n",
      "- Mejor val_loss: 0.755270\n",
      "\n",
      "======================================================================\n",
      "FASE 2: FINE-TUNING PROGRESIVO\n",
      "======================================================================\n",
      "\n",
      "--- Etapa 1: Fine-tuning de capas Dense ---\n",
      "Epoch 1/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 466ms/step - accuracy: 0.5389 - loss: 0.7099 - val_accuracy: 0.0000e+00 - val_loss: 0.7531 - learning_rate: 5.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5820 - loss: 0.7055\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.5888 - loss: 0.7053 - val_accuracy: 0.0000e+00 - val_loss: 0.7780 - learning_rate: 5.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5834 - loss: 0.7027\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.5827 - loss: 0.7034 - val_accuracy: 0.0000e+00 - val_loss: 0.7888 - learning_rate: 2.5000e-04\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "--- Etapa 2: Fine-tuning de LSTM + Dense ---\n",
      "Epoch 1/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 437ms/step - accuracy: 0.5085 - loss: 0.9712 - val_accuracy: 0.0000e+00 - val_loss: 0.7440 - learning_rate: 3.0000e-04\n",
      "Epoch 2/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 0.5268 - loss: 0.9342 - val_accuracy: 0.0000e+00 - val_loss: 0.7396 - learning_rate: 3.0000e-04\n",
      "Epoch 3/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - accuracy: 0.5450 - loss: 0.8904 - val_accuracy: 0.0000e+00 - val_loss: 0.7336 - learning_rate: 3.0000e-04\n",
      "Epoch 4/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4810 - loss: 0.9803\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - accuracy: 0.5109 - loss: 0.9107 - val_accuracy: 0.0000e+00 - val_loss: 0.7345 - learning_rate: 3.0000e-04\n",
      "Epoch 5/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5229 - loss: 0.8605\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.5523 - loss: 0.8545 - val_accuracy: 0.0000e+00 - val_loss: 0.7355 - learning_rate: 1.5000e-04\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "--- Etapa 3: Fine-tuning de CNN final + LSTM + Dense ---\n",
      "Epoch 1/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 467ms/step - accuracy: 0.5365 - loss: 0.8948 - val_accuracy: 0.0000e+00 - val_loss: 0.7525 - learning_rate: 1.0000e-04\n",
      "Epoch 2/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.5178 - loss: 0.9359\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.5170 - loss: 0.9388 - val_accuracy: 0.0000e+00 - val_loss: 0.7585 - learning_rate: 1.0000e-04\n",
      "Epoch 3/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5059 - loss: 0.9555\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.5085 - loss: 0.9610 - val_accuracy: 0.0000e+00 - val_loss: 0.7639 - learning_rate: 5.0000e-05\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "--- Etapa 4: Fine-tuning completo (LR muy bajo) ---\n",
      "Epoch 1/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 544ms/step - accuracy: 0.5122 - loss: 1.1691 - val_accuracy: 0.8493 - val_loss: 0.7438 - learning_rate: 5.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 248ms/step - accuracy: 0.4988 - loss: 1.2634 - val_accuracy: 0.8493 - val_loss: 0.7212 - learning_rate: 5.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 253ms/step - accuracy: 0.5195 - loss: 1.1715 - val_accuracy: 0.8767 - val_loss: 0.7003 - learning_rate: 5.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 251ms/step - accuracy: 0.5146 - loss: 1.1417 - val_accuracy: 0.9658 - val_loss: 0.6794 - learning_rate: 5.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 279ms/step - accuracy: 0.5073 - loss: 1.2029 - val_accuracy: 1.0000 - val_loss: 0.6605 - learning_rate: 5.0000e-05\n",
      "Epoch 6/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 248ms/step - accuracy: 0.4830 - loss: 1.2325 - val_accuracy: 1.0000 - val_loss: 0.6450 - learning_rate: 5.0000e-05\n",
      "Epoch 7/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.5328 - loss: 1.1415 - val_accuracy: 1.0000 - val_loss: 0.6316 - learning_rate: 5.0000e-05\n",
      "Epoch 8/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 248ms/step - accuracy: 0.4988 - loss: 1.1457 - val_accuracy: 1.0000 - val_loss: 0.6171 - learning_rate: 5.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Fine-tuning progresivo completado:\n",
      "- Épocas totales ejecutadas: 19\n",
      "- Épocas máximas configuradas: 26\n",
      "\n",
      "Resultados para sujeto 0:\n",
      "- Accuracy: 0.5000\n",
      "- F1-score: 0.3333\n",
      "- Cohen's κ: 0.0000\n",
      "- Tiempo total: 2.29 minutos\n",
      "\n",
      "======================================================================\n",
      "ITERACIÓN 2/2: Sujeto 1 como prueba\n",
      "======================================================================\n",
      "- Sujetos de entrenamiento: [0]\n",
      "- Sujeto de prueba: 1\n",
      "- Trials entrenamiento: 44\n",
      "- Trials prueba: 44\n",
      "Aplicando data augmentation (factor: 1x)...\n",
      "Métodos: gaussian_noise, amplitude_scale, time_warp, time_shift\n",
      "Original: 44 muestras\n",
      "Aumentado: 88 muestras\n",
      "Factor: 2.0x\n",
      "\n",
      "======================================================================\n",
      "FASE 1: PRE-ENTRENAMIENTO MULTI-SUJETO\n",
      "======================================================================\n",
      "Entrenando modelo con 968 ventanas de 2 sujetos...\n",
      "Learning rate inicial: 0.001\n",
      "Épocas máximas: 30\n",
      "Epoch 1/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 624ms/step - accuracy: 0.5158 - loss: 1.3590 - val_accuracy: 0.0000e+00 - val_loss: 0.7706 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 267ms/step - accuracy: 0.5122 - loss: 1.3015 - val_accuracy: 0.2055 - val_loss: 0.7781 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 256ms/step - accuracy: 0.4830 - loss: 1.3499 - val_accuracy: 0.0205 - val_loss: 0.7886 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.4793 - loss: 1.2635\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - accuracy: 0.4854 - loss: 1.2196 - val_accuracy: 0.0000e+00 - val_loss: 0.8166 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 252ms/step - accuracy: 0.5231 - loss: 1.1556 - val_accuracy: 0.0000e+00 - val_loss: 0.8340 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 259ms/step - accuracy: 0.5085 - loss: 1.2218 - val_accuracy: 0.0000e+00 - val_loss: 0.8516 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Pre-entrenamiento completado:\n",
      "- Épocas ejecutadas: 6/30\n",
      "- Mejor val_loss: 0.770643\n",
      "\n",
      "======================================================================\n",
      "FASE 2: FINE-TUNING PROGRESIVO\n",
      "======================================================================\n",
      "\n",
      "--- Etapa 1: Fine-tuning de capas Dense ---\n",
      "Epoch 1/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 353ms/step - accuracy: 0.5280 - loss: 0.7130 - val_accuracy: 0.0000e+00 - val_loss: 0.7718 - learning_rate: 5.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5929 - loss: 0.7017\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.5852 - loss: 0.7051 - val_accuracy: 0.0000e+00 - val_loss: 0.8006 - learning_rate: 5.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5846 - loss: 0.6980\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.5815 - loss: 0.6997 - val_accuracy: 0.0000e+00 - val_loss: 0.8127 - learning_rate: 2.5000e-04\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "--- Etapa 2: Fine-tuning de LSTM + Dense ---\n",
      "Epoch 1/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 532ms/step - accuracy: 0.5316 - loss: 1.0219 - val_accuracy: 0.0000e+00 - val_loss: 0.7637 - learning_rate: 3.0000e-04\n",
      "Epoch 2/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.5195 - loss: 1.0400 - val_accuracy: 0.0000e+00 - val_loss: 0.7587 - learning_rate: 3.0000e-04\n",
      "Epoch 3/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 0.4939 - loss: 1.0079 - val_accuracy: 0.0000e+00 - val_loss: 0.7516 - learning_rate: 3.0000e-04\n",
      "Epoch 4/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.5584 - loss: 0.8889 - val_accuracy: 0.0000e+00 - val_loss: 0.7479 - learning_rate: 3.0000e-04\n",
      "Epoch 5/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.5243 - loss: 0.9714 - val_accuracy: 0.0000e+00 - val_loss: 0.7462 - learning_rate: 3.0000e-04\n",
      "Epoch 6/6\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5266 - loss: 0.9569\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.5122 - loss: 0.9603 - val_accuracy: 0.0000e+00 - val_loss: 0.7474 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "--- Etapa 3: Fine-tuning de CNN final + LSTM + Dense ---\n",
      "Epoch 1/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 539ms/step - accuracy: 0.5122 - loss: 1.0094 - val_accuracy: 0.0000e+00 - val_loss: 0.7599 - learning_rate: 1.0000e-04\n",
      "Epoch 2/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.5146 - loss: 0.9846 - val_accuracy: 0.0000e+00 - val_loss: 0.7586 - learning_rate: 1.0000e-04\n",
      "Epoch 3/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.5134 - loss: 0.9965 - val_accuracy: 0.0000e+00 - val_loss: 0.7549 - learning_rate: 1.0000e-04\n",
      "Epoch 4/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.5263 - loss: 0.9820\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy: 0.5304 - loss: 0.9422 - val_accuracy: 0.0000e+00 - val_loss: 0.7553 - learning_rate: 1.0000e-04\n",
      "Epoch 5/7\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.5284 - loss: 0.9704\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5195 - loss: 0.9649 - val_accuracy: 0.0000e+00 - val_loss: 0.7588 - learning_rate: 5.0000e-05\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "--- Etapa 4: Fine-tuning completo (LR muy bajo) ---\n",
      "Epoch 1/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 711ms/step - accuracy: 0.4781 - loss: 1.4140 - val_accuracy: 0.3767 - val_loss: 0.7592 - learning_rate: 5.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 305ms/step - accuracy: 0.4964 - loss: 1.3699 - val_accuracy: 0.3767 - val_loss: 0.7546 - learning_rate: 5.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 302ms/step - accuracy: 0.4951 - loss: 1.3800 - val_accuracy: 0.3767 - val_loss: 0.7533 - learning_rate: 5.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.4903 - loss: 1.3829\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 302ms/step - accuracy: 0.5024 - loss: 1.3357 - val_accuracy: 0.3767 - val_loss: 0.7643 - learning_rate: 5.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.5114 - loss: 1.3714\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 302ms/step - accuracy: 0.5207 - loss: 1.3291 - val_accuracy: 0.3767 - val_loss: 0.7722 - learning_rate: 2.5000e-05\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Fine-tuning progresivo completado:\n",
      "- Épocas totales ejecutadas: 19\n",
      "- Épocas máximas configuradas: 26\n",
      "\n",
      "Resultados para sujeto 1:\n",
      "- Accuracy: 0.5000\n",
      "- F1-score: 0.3333\n",
      "- Cohen's κ: 0.0000\n",
      "- Tiempo total: 2.18 minutos\n",
      "\n",
      "======================================================================\n",
      "RESULTADOS FINALES - LEAVE-ONE-SUBJECT-OUT\n",
      "======================================================================\n",
      "\n",
      "Métricas Promedio:\n",
      "- Accuracy: 0.5000 ± 0.0000\n",
      "- Precision: 0.2500\n",
      "- Recall: 0.5000\n",
      "- F1-score: 0.3333 ± 0.0000\n",
      "- Cohen's κ: 0.0000\n",
      "\n",
      "Tiempo total: 4.46 minutos (0.07 horas)\n",
      "\n",
      "======================================================================\n",
      "PROCESO COMPLETADO\n",
      "======================================================================\n",
      "Tiempo total de ejecución: 4.93 minutos (0.08 horas)\n",
      "Todos los resultados han sido calculados\n"
     ]
    }
   ],
   "source": [
    "# Configuración de número de sujetos a procesar\n",
    "# Para pruebas rápidas: MAX_SUBJECTS = 2\n",
    "# Para ejecución completa: MAX_SUBJECTS = None (procesa todos los sujetos)\n",
    "MAX_SUBJECTS = 2  # Por defecto: modo prueba rápida\n",
    "\n",
    "# Cargar datos con rastreo de sujetos\n",
    "print(\"=\"*70)\n",
    "print(\"CARGANDO DATOS CON RASTREO DE SUJETOS\")\n",
    "print(\"=\"*70)\n",
    "X, y, subject_ids = load_eeg_data_with_subjects(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "print(f\"\\nDatos cargados (antes del filtrado):\")\n",
    "print(f\"- Shape X: {X.shape}\")\n",
    "print(f\"- Shape y: {y.shape}\")\n",
    "print(f\"- Shape subject_ids: {subject_ids.shape}\")\n",
    "print(f\"- Sujetos únicos: {len(np.unique(subject_ids))}\")\n",
    "\n",
    "# Filtrar sujetos si MAX_SUBJECTS está configurado\n",
    "if MAX_SUBJECTS is not None:\n",
    "    unique_subjects = np.unique(subject_ids)\n",
    "    selected_subjects = sorted(unique_subjects)[:MAX_SUBJECTS]\n",
    "    \n",
    "    # Crear máscara para filtrar solo los sujetos seleccionados\n",
    "    mask = np.isin(subject_ids, selected_subjects)\n",
    "    \n",
    "    # Filtrar datos\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    subject_ids = subject_ids[mask]\n",
    "    \n",
    "    # Remapear IDs de sujetos para que sean consecutivos desde 0\n",
    "    subject_id_mapping = {old_id: new_id for new_id, old_id in enumerate(selected_subjects)}\n",
    "    subject_ids = np.array([subject_id_mapping[sid] for sid in subject_ids])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MODO PRUEBA: Procesando solo {MAX_SUBJECTS} sujetos\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sujetos seleccionados: {selected_subjects}\")\n",
    "    print(f\"Para ejecución completa, cambiar MAX_SUBJECTS = None\")\n",
    "else:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODO COMPLETO: Procesando todos los sujetos\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nDatos cargados (después del filtrado):\")\n",
    "print(f\"- Shape X: {X.shape}\")\n",
    "print(f\"- Shape y: {y.shape}\")\n",
    "print(f\"- Shape subject_ids: {subject_ids.shape}\")\n",
    "print(f\"- Sujetos únicos: {len(np.unique(subject_ids))}\")\n",
    "\n",
    "# Ejecutar Leave-One-Subject-Out Cross-Validation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INICIANDO LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "if MAX_SUBJECTS is not None:\n",
    "    print(f\"MODO PRUEBA: Procesando {MAX_SUBJECTS} sujetos (~10-30 minutos)\")\n",
    "else:\n",
    "    print(\"ADVERTENCIA: Este proceso puede tomar varias horas (1-2 horas)\")\n",
    "print(f\"Total de iteraciones: {len(np.unique(subject_ids))}\")\n",
    "print(f\"Cada iteración incluye:\")\n",
    "print(f\"- Pre-entrenamiento con múltiples sujetos\")\n",
    "print(f\"- Fine-tuning progresivo (4 etapas)\")\n",
    "print(f\"- Evaluación\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_results, subject_results, avg_metrics = leave_one_subject_out_cv(\n",
    "    X, y, subject_ids, CONFIG,\n",
    "    use_augmentation=CONFIG.get('use_augmentation', True)\n",
    ")\n",
    "\n",
    "total_time = (time.time() - start_time) / 60  # minutos\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROCESO COMPLETADO\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Tiempo total de ejecución: {total_time:.2f} minutos ({total_time/60:.2f} horas)\")\n",
    "print(\"Todos los resultados han sido calculados\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualización de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VISUALIZACIÓN DE RESULTADOS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TABLA RESUMEN POR SUJETO:\n",
      " subject_id  accuracy  precision  recall  f1_score  cohen_kappa\n",
      "          0       0.5       0.25     0.5  0.333333          0.0\n",
      "          1       0.5       0.25     0.5  0.333333          0.0\n",
      "\n",
      "Visualizaciones guardadas\n"
     ]
    }
   ],
   "source": [
    "# Visualizar resultados por sujeto\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZACIÓN DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con resultados por sujeto\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('subject_id')\n",
    "\n",
    "# Gráfico de barras: Accuracy por sujeto\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(results_df['subject_id'], results_df['accuracy'], alpha=0.7, color='steelblue')\n",
    "plt.axhline(y=avg_metrics['accuracy'], color='r', linestyle='--', \n",
    "            label=f'Promedio: {avg_metrics[\"accuracy\"]:.4f}')\n",
    "plt.xlabel('ID del Sujeto')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy por Sujeto - Leave-One-Subject-Out Cross-Validation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['results_dir']) / 'transfer_learning_loso_accuracy_by_subject.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de cajas: Distribución de métricas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'f1_score', 'cohen_kappa']\n",
    "titles = ['Accuracy', 'F1-Score', \"Cohen's κ\"]\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    axes[idx].boxplot(results_df[metric], vert=True)\n",
    "    axes[idx].set_ylabel(title)\n",
    "    axes[idx].set_title(f'Distribución de {title}')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['results_dir']) / 'transfer_learning_loso_metrics_distribution.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"\\nTABLA RESUMEN POR SUJETO:\")\n",
    "print(results_df[['subject_id', 'accuracy', 'precision', 'recall', 'f1_score', 'cohen_kappa']].to_string(index=False))\n",
    "\n",
    "print(\"\\nVisualizaciones guardadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GUARDANDO RESULTADOS\n",
      "======================================================================\n",
      "Resultados guardados en: results/transfer_learning_loso_results.json\n",
      "Resultados CSV guardados en: results/transfer_learning_loso_results.csv\n",
      "\n",
      "======================================================================\n",
      "NOTEBOOK COMPLETADO EXITOSAMENTE\n",
      "======================================================================\n",
      "\n",
      "Archivos generados:\n",
      "- results/transfer_learning_loso_results.json\n",
      "- results/transfer_learning_loso_results.csv\n",
      "- results/transfer_learning_loso_accuracy_by_subject.png\n",
      "- results/transfer_learning_loso_metrics_distribution.png\n",
      "\n",
      "Resultados promedio:\n",
      "- Accuracy: 0.5000 ± 0.0000\n",
      "- F1-Score: 0.3333 ± 0.0000\n",
      "- Cohen's κ: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Guardar todos los resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear diccionario completo de resultados\n",
    "results_to_save = {\n",
    "    'average_metrics': {\n",
    "        k: float(v) for k, v in avg_metrics.items() if not isinstance(v, (list, dict))\n",
    "    },\n",
    "    'per_subject_results': {\n",
    "        str(k): {m: float(v) for m, v in metrics.items() if isinstance(v, (int, float))}\n",
    "        for k, metrics in subject_results.items()\n",
    "    },\n",
    "    'all_results': [\n",
    "        {k: float(v) if isinstance(v, (int, float)) else v for k, v in r.items()}\n",
    "        for r in all_results\n",
    "    ],\n",
    "    'config': CONFIG,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Guardar JSON\n",
    "results_json_path = Path(CONFIG['results_dir']) / 'transfer_learning_loso_results.json'\n",
    "with open(results_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Resultados guardados en: {results_json_path}\")\n",
    "\n",
    "# Guardar CSV\n",
    "results_csv_path = Path(CONFIG['results_dir']) / 'transfer_learning_loso_results.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Resultados CSV guardados en: {results_csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nArchivos generados:\")\n",
    "print(f\"- {results_json_path}\")\n",
    "print(f\"- {results_csv_path}\")\n",
    "print(f\"- {Path(CONFIG['results_dir']) / 'transfer_learning_loso_accuracy_by_subject.png'}\")\n",
    "print(f\"- {Path(CONFIG['results_dir']) / 'transfer_learning_loso_metrics_distribution.png'}\")\n",
    "print(f\"\\nResultados promedio:\")\n",
    "print(f\"- Accuracy: {avg_metrics['accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n",
    "print(f\"- F1-Score: {avg_metrics['f1_score']:.4f} ± {avg_metrics['std_f1']:.4f}\")\n",
    "print(f\"- Cohen's κ: {avg_metrics['cohen_kappa']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
